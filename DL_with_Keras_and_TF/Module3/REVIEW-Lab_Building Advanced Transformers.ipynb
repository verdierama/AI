{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Building Advanced Transformers**\n",
    "\n",
    "**Estimated time needed:  30 minutes**  \n",
    "\n",
    "In this lab, you will implement and experiment with advanced Transformer models using Keras. \n",
    "\n",
    "**Learning objectives:** \n",
    "\n",
    "By the end of this lab, you will: \n",
    "\n",
    "- Implement advanced Transformer models using Keras. \n",
    "\n",
    "- Apply Transformers to real-world sequential data tasks. \n",
    "\n",
    "- Build, train, and evaluate Transformer models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions: \n",
    "\n",
    "### Step 1: Import necessary libraries \n",
    "\n",
    "Before you start, you need to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.0-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
      "  Downloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.4/615.4 MB\u001b[0m \u001b[31m842.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.0-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (391 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m391.8/391.8 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, pyarrow, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.1 h5py-3.12.1 keras-3.7.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 opt-einsum-3.4.0 optree-0.13.1 protobuf-5.29.0 pyarrow-18.1.0 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 werkzeug-3.1.3 wrapt-1.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m125.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (2.0.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m16.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.55.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (164 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.7/164.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (24.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.2/326.2 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.1 kiwisolver-1.4.7 matplotlib-3.9.3 pillow-11.0.0 pyparsing-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow pyarrow \n",
    "%pip install pandas  \n",
    "%pip install scikit-learn \n",
    "%pip install matplotlib \n",
    "%pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 10:36:12.291255: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-04 10:36:12.293457: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-04 10:36:12.297013: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-04 10:36:12.308236: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733308572.327264      84 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733308572.332898      84 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 10:36:12.354609: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setup the Environment to generate synthetic stock price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic stock_prices.csv created and loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a synthetic stock price dataset\n",
    "np.random.seed(42)\n",
    "data_length = 2000  # Adjust data length as needed\n",
    "trend = np.linspace(100, 200, data_length)\n",
    "noise = np.random.normal(0, 2, data_length)\n",
    "synthetic_data = trend + noise\n",
    "\n",
    "# Create a DataFrame and save as 'stock_prices.csv'\n",
    "data = pd.DataFrame(synthetic_data, columns=['Close'])\n",
    "data.to_csv('stock_prices.csv', index=False)\n",
    "print(\"Synthetic stock_prices.csv created and loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1899, 100, 1)\n",
      "Shape of Y: (1899,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "data = pd.read_csv('stock_prices.csv') \n",
    "data = data[['Close']].values \n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for training\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 100\n",
    "X, Y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`tensorflow` is the main library for machine learning in Python.  \n",
    "\n",
    "`stock_prices.csv` is the data set that is loaded. \n",
    "\n",
    "`MinMaxScaler` method is used to normalize the data.  \n",
    "\n",
    "`create_dataset`method is used to prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Multi-Head Self-Attention \n",
    "\n",
    "Define the Multi-Head Self-Attention mechanism. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    "\n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The MultiHeadSelfAttention layer implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. \n",
    "\n",
    "- The attention parameter computes the attention scores and weighted sum of the values. \n",
    "\n",
    "- The split_heads parameter splits the input into multiple heads for parallel attention computation. \n",
    "\n",
    "- The call method applies the self-attention mechanism and combines the heads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Transformer block \n",
    "\n",
    "Define the Transformer block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "\n",
    "- The TransformerBlock layer combines multi-head self-attention with a feed-forward neural network and normalization layers.  \n",
    "\n",
    "- Dropout is used to prevent overfitting. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network with residual connections and layer normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Encoder Layer \n",
    "\n",
    "Define the Encoder layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(EncoderLayer, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The EncoderLayer is similar to the TransformerBlock but is a reusable layer in the Transformer architecture. \n",
    "\n",
    "- It consists of a MultiHeadSelfAttention mechanism followed by a feedforward neural network. \n",
    "\n",
    "- Both sub-layers have residual connections around them, and layer normalization is applied to the output of each sub-layer. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network, with residual connections and layer normalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement Transformer encoder \n",
    "\n",
    "Define the Transformer Encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 10:37:16.741947: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout \n",
    "\n",
    "class MultiHeadSelfAttention(Layer): \n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    " \n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    "class TransformerBlock(Layer): \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n",
    "class TransformerEncoder(Layer): \n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.embed_dim = embed_dim \n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n",
    "        self.dropout = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs \n",
    "        for i in range(self.num_layers): \n",
    "            x = self.enc_layers[i](x, training=training) \n",
    "        return x \n",
    "\n",
    "# Example usage \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "inputs = tf.random.uniform((1, 100, embed_dim)) \n",
    "outputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training' \n",
    "print(outputs.shape)  # Should print (1, 100, 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The TransformerEncoder is composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build and Compile the Transformer model \n",
    "\n",
    "Integrate the Transformer Encoder into a complete model for sequential data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_74 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_75 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the necessary parameters \n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The Transformer Encoder model defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output.  \n",
    "\n",
    "- The model is then compiled with the Adam optimizer and mean squared error loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the Transformer model \n",
    "\n",
    "Train the model on the prepared dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 996ms/step - loss: 11.9300\n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 980ms/step - loss: 0.2012\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 982ms/step - loss: 0.1710\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 973ms/step - loss: 0.1719\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 975ms/step - loss: 0.1397\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 976ms/step - loss: 0.1636\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 969ms/step - loss: 0.1464\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 963ms/step - loss: 0.1160\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 972ms/step - loss: 0.1496\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 973ms/step - loss: 0.1240\n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 967ms/step - loss: 0.0992\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 965ms/step - loss: 0.0995\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 962ms/step - loss: 0.0865\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 965ms/step - loss: 0.0751\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 967ms/step - loss: 0.1023\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 988ms/step - loss: 0.0884\n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0622\n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 973ms/step - loss: 0.0378\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 966ms/step - loss: 0.0435\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 982ms/step - loss: 0.0342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f90ec616e10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The model is trained on the normalized stock price data for 20 epochs with a batch size of 32. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate and Make Predictions \n",
    "\n",
    "Evaluate the model's performance and make predictions on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 291ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions \n",
    "predictions = model.predict(X) \n",
    "predictions = scaler.inverse_transform(predictions) \n",
    " \n",
    "\n",
    "# Plot the predictions \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#plt.plot(data, label='True Data') \n",
    "plt.plot(scaler.inverse_transform(data), label='True Data') \n",
    "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions') \n",
    "plt.xlabel('Time') \n",
    "plt.ylabel('Stock Price') \n",
    "plt.legend() \n",
    "plt.show() \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The model's predictions are transformed back to the original scale using the inverse transform of the scaler. \n",
    "\n",
    "- The true data and predictions are plotted to visualize the model's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises: \n",
    "\n",
    " ### Exercise 1: Add dropout to the Transformer model \n",
    "\n",
    " **Objective: Understand how to add dropout layers to the Transformer model to prevent overfitting.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Add a dropout layer after the Flatten layer in the model. \n",
    "\n",
    "- Set the dropout rate to 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 972ms/step - loss: 15.1103\n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 966ms/step - loss: 1.9542\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 961ms/step - loss: 1.6357\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 977ms/step - loss: 1.2676\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 971ms/step - loss: 0.7682\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 965ms/step - loss: 0.4727\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 972ms/step - loss: 0.1513\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 969ms/step - loss: 0.0713\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 985ms/step - loss: 0.0409\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 981ms/step - loss: 0.0317\n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 980ms/step - loss: 0.0257\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 981ms/step - loss: 0.0231\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 985ms/step - loss: 0.0220\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 979ms/step - loss: 0.0269\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 977ms/step - loss: 0.0219\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 982ms/step - loss: 0.0169\n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 985ms/step - loss: 0.0200\n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 974ms/step - loss: 0.0177\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 978ms/step - loss: 0.0167\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 975ms/step - loss: 0.0209\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 284ms/step - loss: 5.9942e-04\n",
      "Test loss: 0.0005441343528218567\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "  \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') \n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with different batch sizes \n",
    "\n",
    "**Objective: Observe the impact of different batch sizes on model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Train the model with a batch size of 16. \n",
    "\n",
    "- Train the model with a batch size of 64. \n",
    "\n",
    "- Compare the training time and performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 567ms/step - loss: 0.0249\n",
      "Epoch 2/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 568ms/step - loss: 0.0321\n",
      "Epoch 3/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 569ms/step - loss: 0.0328\n",
      "Epoch 4/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 572ms/step - loss: 0.0269\n",
      "Epoch 5/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 565ms/step - loss: 0.0284\n",
      "Epoch 6/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 568ms/step - loss: 0.0192\n",
      "Epoch 7/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 569ms/step - loss: 0.0278\n",
      "Epoch 8/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 572ms/step - loss: 0.0187\n",
      "Epoch 9/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 565ms/step - loss: 0.0275\n",
      "Epoch 10/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 565ms/step - loss: 0.0221\n",
      "Epoch 11/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 569ms/step - loss: 0.0173\n",
      "Epoch 12/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 571ms/step - loss: 0.0160\n",
      "Epoch 13/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 563ms/step - loss: 0.0155\n",
      "Epoch 14/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 562ms/step - loss: 0.0275\n",
      "Epoch 15/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 569ms/step - loss: 0.0175\n",
      "Epoch 16/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 564ms/step - loss: 0.0147\n",
      "Epoch 17/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 562ms/step - loss: 0.0115\n",
      "Epoch 18/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 565ms/step - loss: 0.0090\n",
      "Epoch 19/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 568ms/step - loss: 0.0158\n",
      "Epoch 20/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 567ms/step - loss: 0.0100\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 282ms/step - loss: 8.4910e-04\n",
      "Test loss for batch of 16: 0.0009161712951026857\n",
      "Epoch 1/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 2s/step - loss: 0.0068\n",
      "Epoch 2/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 2s/step - loss: 0.0054\n",
      "Epoch 3/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - loss: 0.0049\n",
      "Epoch 4/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2s/step - loss: 0.0043\n",
      "Epoch 5/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - loss: 0.0047\n",
      "Epoch 6/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2s/step - loss: 0.0043\n",
      "Epoch 7/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - loss: 0.0041\n",
      "Epoch 8/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - loss: 0.0043\n",
      "Epoch 9/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - loss: 0.0042\n",
      "Epoch 10/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - loss: 0.0042\n",
      "Epoch 11/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2s/step - loss: 0.0040\n",
      "Epoch 12/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2s/step - loss: 0.0046\n",
      "Epoch 13/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2s/step - loss: 0.0048\n",
      "Epoch 14/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2s/step - loss: 0.0044\n",
      "Epoch 15/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2s/step - loss: 0.0041\n",
      "Epoch 16/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2s/step - loss: 0.0038\n",
      "Epoch 17/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 2s/step - loss: 0.0040\n",
      "Epoch 18/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2s/step - loss: 0.0042\n",
      "Epoch 19/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0041\n",
      "Epoch 20/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2s/step - loss: 0.0047\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 289ms/step - loss: 5.0761e-04\n",
      "Test loss for batch of 64: 0.0008699700701981783\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "\n",
    "# Train the model \n",
    "model.fit(X, Y, epochs=20, batch_size=16) \n",
    "\n",
    "# Evaluate the model \n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss for batch of 16: {loss}') \n",
    "\n",
    "# Train the model \n",
    "model.fit(X, Y, epochs=20, batch_size=64) \n",
    "\n",
    "# Evaluate the model \n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss for batch of 64: {loss}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different activation function \n",
    "\n",
    " **Objective: Understand how different activation functions impact the model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Change the activation function of the Dense layer to `tanh`. \n",
    "\n",
    "- Train and evaluate the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 993ms/step - loss: 0.2863\n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 980ms/step - loss: 0.2923\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 985ms/step - loss: 0.1646\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 987ms/step - loss: 0.0950\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 994ms/step - loss: 0.0113\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 983ms/step - loss: 0.0071\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 993ms/step - loss: 0.0051\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 988ms/step - loss: 0.0048\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 984ms/step - loss: 0.0058\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 986ms/step - loss: 0.0044\n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - loss: 0.0026\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 990ms/step - loss: 0.0022\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 994ms/step - loss: 0.0022\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 982ms/step - loss: 0.0022\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 977ms/step - loss: 0.0019\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 982ms/step - loss: 0.0018\n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 981ms/step - loss: 0.0019\n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 992ms/step - loss: 0.0022\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 996ms/step - loss: 0.0019\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 979ms/step - loss: 0.0016\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 284ms/step - loss: 7.9164e-04\n",
      "Test loss  witn tanh: 0.0011355181923136115\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "# Train the model \n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "# Evaluate the model \n",
    "loss = model.evaluate(X, Y) \n",
    "print(f'Test loss  witn tanh: {loss}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this lab! In this lab, you have built an advanced Transformer model using Keras and applied it to a time series forecasting task. You have learned how to define and implement multi-head self-attention, Transformer blocks, encoder layers, and integrate them into a complete Transformer model. By experimenting with different configurations and training the model, you can further improve its performance and apply it to various sequential data tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "28ac4fd81c1d713f83dcd1cdf1d3383ad25ea92873288fe9e978e9a17b314709"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
