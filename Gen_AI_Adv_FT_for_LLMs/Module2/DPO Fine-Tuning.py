#!/usr/bin/env python
# coding: utf-8

# <p style="text-align:center">
#     <a href="https://skills.network" target="_blank">
#     <img src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png" width="200" alt="Skills Network Logo"  />
#     </a>
# </p>
# 

# # **Direct Preference Optimization (DPO) Using Hugging Face**
# 

# Estimated time needed: **60** minutes
# 

# Large language models (LLMs) have revolutionized the field of natural language processing (NLP) by achieving remarkable performance in various tasks. However, it is challenging to align these models with human preferences. Therefore, the direct preference optimization (DPO) method comes in place which directly optimizes LLMs based models on user preferences, enhancing their alignment with human expectations. In this hands-on lab, you'll use the transformer reinforcement learning (trl) library from Hugging Face to implement DPO and fine-tune LLMs.
# 
# The objective of this lab is to provide a practical understanding of the DPO method and its implementation using the trl library. 
# 
# By the end of this lab, you'll have hands-on experience in creating a data set formatted for DPO, implementing the optimization process, and evaluating the enhanced performance of LLMs.
# 

# ## __Table of Contents__
# 
# <ol>
#     <li><a href="#Objectives">Objectives</a></li>
#     <li>
#         <a href="#Setup">Setup</a>
#         <ol>
#             <li><a href="#Installing-required-libraries">Installing required libraries</a></li>
#             <li><a href="#Importing-required-libraries">Importing required libraries</a></li>
#         </ol>
#     </li>
#     <li>
#         <a href="#Create-and-configure-the-model-and-tokenizer">Create and configure the model and tokenizer</a>
#         <ol>
#             <li><a href="#Quantized-model-configuration-(Optional)">Quantized model configuration (Optional)</a></li>
#         </ol>
#     </li>
#     <li><a href="#Preprocess-dataset">Preprocess dataset</a></li>
#     <li><a href="#DPO-configuration">DPO configuration</a></li>
#     <li><a href="#DPO-training">DPO training</a></li>
#     <li><a href="#Exercise">Exercise</a>
# </ol>
#    
# 

# ## Objectives
# 
# After completing this lab, you'll be able to: 
# - Understand the fundamentals of DPO and how it is different from proximal policy optimization (PPO)
# - Set up an environment by installing and configuring necessary tools and libraries, such as trl library from Hugging Face
# - Prepare a suitable environment for running DPO experiments with LLMs
# - Create a data set for DPO
# - Understand the required format for data sets used in DPO
# - Create and preprocess a data set that includes user preferences
# - Implement DPO by following a step-by-step guideline using the trl library
# - Set training arguments, create a base quantized LoRA model, and train it using a DPO trainer
# - Evaluate the performance of the LLM before and after applying DPO
# - Analyze the impact of DPO on aligning the model with user preferences
# 
# By the end of this hands-on lab, you will be equipped with the knowledge and skills needed to apply DPO for fine-tuning LLMs using the trl library. This will enable you to enhance LLMs' performance and user alignment in various NLP applications.
# 

# ----
# 

# ## Setup
# 

# ### Installing required libraries
# 

# The following required libraries are __not__ pre-installed in the Skills Network Labs environment. You will need to run the following cell to install them.
# 
# **Note:** In this lab, you don't have a pinned version to demonstrate the latest functionality, but you can always pin versions in your labs.
# 

# In[2]:


#get_ipython().system('pip install torch')
#get_ipython().system('pip install trl # for optimization training')
#get_ipython().system('pip install peft # for creating LoRA architecture')
#get_ipython().system('pip install matplotlib')


# ### Importing required libraries
# 
# _It's recommended to import all required libraries in one place (here):_
# 

# In[20]:


#AV!pip install pandas
#AV!/opt/conda/bin/python -m pip install datasets
#AVimport datasets
#AV!pip install trl
#AVimport transformers
#AV!pip uninstall peft
#AV!pip uninstall trl
#get_ipython().system('pip install peft==0.11.1')
#get_ipython().system('/opt/conda/bin/pip uninstall trl -y')
#get_ipython().system('/opt/conda/bin/python -m pip install trl==0.9.6')
import peft
import trl
import transformers
#print(trl.__version__)
#print(peft.__version__)
#print(transformers.__version__)

##imports
import multiprocessing
import os
import requests
import tarfile
import pandas as pd
import matplotlib.pyplot as plt

import torch
from datasets import load_dataset

from peft import LoraConfig
from transformers import AutoModelForCausalLM, AutoTokenizer,TrainingArguments, GPT2Tokenizer, set_seed, GenerationConfig
from trl import DPOConfig, DPOTrainer


# ## Create and configure the model and tokenizer
# 

# In[7]:


# Load the GPT-2 model
model = AutoModelForCausalLM.from_pretrained("gpt2")

# Load a reference model 
model_ref = AutoModelForCausalLM.from_pretrained("gpt2")

# Load the GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Set the pad token to the end-of-sequence token
tokenizer.pad_token = tokenizer.eos_token
# Set the padding side to "right" to fix the overflow issue with FP16 training
tokenizer.padding_side = "right"

# Disable the use of the cache during the model's forward pass
model.config.use_cache = False


# Here, you can check the model architecture.
# 

# In[9]:


print("--model: ",model)


# ### Quantized model configuration (Optional)
# If you want memory-efficient training and have access to a GPU-powered environment, you can download the complete lab, uncomment the following two code blocks to create a quantized model and proceed with training the model on GPU. This is because you will need GPUs for the bits and bytes package.
# 

# In[10]:


#!pip install -U bitsandbytes # this package is required for quantization


# **_Note:_**  _You can run the installed package by restarting a Kernel._
# 

# In[11]:


'''## Quantized model --only available on GPU
from transformers import BitsAndBytesConfig

# Configure the quantization parameters
quantization_config = BitsAndBytesConfig(
    # Load the model in 4-bit quantized format
    load_in_4bit=True,
    # Enable double quantization for better accuracy
    bnb_4bit_use_double_quant=True,
    # Use non-uniform 4-bit quantization (nf4)
    bnb_4bit_quant_type="nf4",
    # Use bfloat16 as the computation data type during quantization
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load GPT-2 model with the specified quantization configuration
model = AutoModelForCausalLM.from_pretrained("gpt2", quantization_config=quantization_config)

# Load a reference model with the same quantization configuration
model_ref = AutoModelForCausalLM.from_pretrained("gpt2", quantization_config=quantization_config)

# Load GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Set the pad token to the end-of-sequence token
tokenizer.pad_token = tokenizer.eos_token
# Set the padding side to "right" to fix the overflow issue with FP16 training
tokenizer.padding_side = "right"

# Disable the use of the cache during the model's forward pass
model.config.use_cache = False'''


# ## Preprocess data set
# 
# The "ultrafeedback_binarized" data set on Hugging Face is a collection of prompts and responses.  
# 

# In[12]:


# Load the dataset from the specified location
print("loading dataset")
ds = load_dataset("BarraHome/ultrafeedback_binarized")


# This data set includes six splits. 
# 

# In[13]:


print("--ds keys : ",ds.keys())


# Each record has different features among which you need to select from the three features, that is "chosen," "rejected," and "prompt." This means that for each prompt, a prefered response and a rejected response are provided.
# 

# In[14]:


print("--train keys : ", ds["train_prefs"][0].keys())


# You can check the sample record of data, where you can see three features along with other features that is the prompt, the rejected, and chosen responses.
# 

# In[15]:


print("--train_prefs : ", ds["train_prefs"][0])


# Now, put the data set in the format that the DPO trainer accepts.
# 
# | Chosen | Rejected | Prompt |
# | --- | --- | --- |
#  | Developing a daily habit of drawing can be challenging <br>but with consistent practice, and a few tips. | One way to develop a habit of drawing daily is <br>to allocate a specific time interval for drawing. | How can I develop a habit of drawing daily?|
# 

# In[16]:


# You can reduce the volume of data (due to resource limitations) by selecting the first 5% examples from each split of the dataset
for key in ds:
    #cnt = round(ds[key].__len__()*0.05)
    cnt=48
    ds[key] = ds[key].select(range(cnt))

# Define a function to process the data
'''def process(row):
    # delete unwanted columns
    del row["prompt_id"]
    del row["messages"]
    del row["score_chosen"]
    del row["score_rejected"]
    # retrieve the actual response text
    row["chosen"] = row["chosen"][-1]["content"]
    row["rejected"] = row["rejected"][-1]["content"]

    return row'''


def process(row):
    # Handle missing keys gracefully
    for key in ["prompt_id", "messages", "score_chosen", "score_rejected"]:
        if key in row:
            del row[key]

    # Process "chosen" and "rejected" fields safely
    if isinstance(row.get("chosen"), list) and len(row["chosen"]) > 0:
        row["chosen"] = row["chosen"][-1].get("content", "")
    else:
        row["chosen"] = ""

    if isinstance(row.get("rejected"), list) and len(row["rejected"]) > 0:
        row["rejected"] = row["rejected"][-1].get("content", "")
    else:
        row["rejected"] = ""

    return row

# Apply the data processing function to the dataset
print("multiprocessing.cpu_count() = ",multiprocessing.cpu_count())
row = ds["train_prefs"][0]
print(process(row))

'''
ds = ds.map(
    process,
    num_proc=multiprocessing.cpu_count(),
    load_from_cache_file=False,
)

# Split the dataset into training and evaluation sets
train_dataset = ds['train_prefs']
eval_dataset = ds['test_prefs']


'''

from datasets import Dataset

# Convertir en liste de dictionnaires
data_list = ds["train_prefs"].to_list()

# Appliquer la transformation
processed_data = [process(row) for row in data_list]

# Reconvertir en Dataset
train_dataset = Dataset.from_dict({key: [row[key] for row in processed_data] for key in processed_data[0]})

# Convertir en liste de dictionnaires
data_list = ds["test_prefs"].to_list()

# Appliquer la transformation
processed_data = [process(row) for row in data_list]

# Reconvertir en Dataset
eval_dataset = Dataset.from_dict({key: [row[key] for row in processed_data] for key in processed_data[0]})


# Let's check the data record.
# 

# In[17]:


print("train dataset : ",train_dataset[0])


# Next, define LoRAConfig for efficient fine-tuning.
# 

# In[18]:


# PEFT (Parameter-Efficient Finetuning) configuration
peft_config = LoraConfig(
        # The rank of the low-rank adaptation weights
        r=4,
        # The target modules to apply the low-rank adaptation to
        target_modules=['c_proj','c_attn'],
        # The task type for the low-rank adaptation
        task_type="CAUSAL_LM",
        # The scaling factor for the low-rank adaptation weights
        lora_alpha=8,
        # The dropout probability for the low-rank adaptation weights
        lora_dropout=0.1,
        # The bias mode for the low-rank adaptation
        bias="none",
)


# ### DPO configuration
# 
# First, define training arguments.
# 

# In[19]:


# DPO configuration
training_args = DPOConfig(
    # The beta parameter for the DPO loss function
    #beta is the temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5 . 
    beta=0.1,
    # The output directory for the training
    output_dir="dpo",
    # The number of training epochs
    num_train_epochs=5,
    # The batch size per device during training
    per_device_train_batch_size=1,
    # The batch size per device during evaluation
    per_device_eval_batch_size=1,
    # Whether to remove unused columns from the dataset
    remove_unused_columns=False,
    # The number of steps between logging training progress
    logging_steps=10,
    # The number of gradient accumulation steps
    gradient_accumulation_steps=1,
    # The learning rate for the optimization
    learning_rate=1e-4,
    # The evaluation strategy (e.g., after each step or epoch)
    #AVevaluation_strategy="epoch",
    eval_strategy="epoch",
    # The number of warmup steps for the learning rate scheduler
    warmup_steps=2,
    # Whether to use 16-bit (float16) precision
    fp16=False,
    # The number of steps between saving checkpoints
    save_steps=500,
    # The maximum number of checkpoints to keep
    #save_total_limit=2,
    # The reporting backend to use (set to 'none' to disable, you can also report to wandb or tensorboard)
    report_to='none'
)


# ### DPO training
# 
# Next step is creating the actual trainer using DPOTrainer class.
# 

# In[26]:


tokenizer.pad_token = tokenizer.eos_token

# Create a DPO trainer
# This trainer will handle the fine-tuning of the model using the DPO technique
trainer = DPOTrainer(
        # The model to be fine-tuned
        model,
        # The reference model (not used in this case because LoRA has been used)
        ref_model=None,
        # The DPO training configuration
        args=training_args,
        # The beta parameter for the DPO loss function
        beta=0.1,
        # The training dataset
        train_dataset=train_dataset,
        # The evaluation dataset
        eval_dataset=eval_dataset,
        # The tokenizer for the model
        tokenizer=tokenizer,
        # The PEFT (Parallel Efficient Finetuning) configuration
        peft_config=peft_config,
        # The maximum prompt length
        max_prompt_length=512,
        # The maximum sequence length
        max_length=512,
    )


# Please note that when using LoRA for the base model, it's efficient to leave the model_ref param null, in which case the DPOTrainer will unload the adapter for reference inference.
# 
# 
# Now, you're all set for training the model.
# 

# #### Training model
# 

# **Keep in mind that training the model on a CPU can be time-consuming and may cause the kernel to crash due to memory issues. If this happens, you can bypass training by loading the pre-trained model provided in the next section and proceed from there.**
# 

# In[30]:


# Start the training process
#AVtrainer.train() 


# Let's retrieve and plot the training loss versus evaluation loss.
# 

# In[31]:


# Retrieve log_history and save it to a dataframe
'''
log = pd.DataFrame(trainer.state.log_history)
log_t = log[log['loss'].notna()]
log_e = log[log['eval_loss'].notna()]

# Plot train and evaluation losses
plt.plot(log_t["epoch"], log_t["loss"], label = "train_loss") 
plt.plot(log_e["epoch"], log_e["eval_loss"], label = "eval_loss") 
plt.legend() 
plt.show()'''


# ![image](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7KEnvtpUyNcJTINdArLf7A/loss%20dpo.png)
# 

# In[33]:


# Load the trained DPO model you just trained
#AVdpo_model = AutoModelForCausalLM.from_pretrained('./dpo/checkpoint-250')


# #### Loading trained model
# 

# If you encounter difficulty in running the training cell due to resource limitations, you can download the model to be fine-tuned: 
# 

# In[34]:


# Define the URL and the filename
url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/YIDeT3qihEpWChdXN_RmTg/DPO-tar.gz'
filename = './DPO.tar'

# Download the file
response = requests.get(url)

# Save the file locally
with open(filename, 'wb') as f:
    f.write(response.content)

# Extract the tar file
if tarfile.is_tarfile(filename):
    with tarfile.open(filename, 'r') as tar:
        tar.extractall()
        print("Files extracted:", tar.getnames())
else:
    print("The downloaded file is not a tar file.")


# Then, load it into the model for further inference:
# 

# In[35]:


# Load the trained DPO model tiy just trained
dpo_model = AutoModelForCausalLM.from_pretrained('./DPO')


# ### Generation
# 

# In[36]:


# Load the GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')


# In[37]:


# Set a seed for reproducibility
set_seed(42)


# Define the generation configuration for the DPO model
# This sets the parameters for text generation
generation_config = GenerationConfig(
        # Use sampling to generate diverse text
        do_sample=True,
        # Top-k sampling parameter
        top_k=1,
        # Temperature parameter to control the randomness of the generated text
        temperature=0.1,
        # Maximum number of new tokens to generate
        max_new_tokens=25,
        # Use the end-of-sequence token as the padding token
        pad_token_id=tokenizer.eos_token_id
    )

# Define the input prompt for text generation
PROMPT = "Is a higher octane gasoline better for your car?"
# Encode the prompt using the tokenizer
inputs = tokenizer(PROMPT, return_tensors='pt')

# Generate text using the DPO model
outputs = dpo_model.generate(**inputs, generation_config=generation_config)
# Decode the generated text and print it
print("DPO response:\t",tokenizer.decode(outputs[0], skip_special_tokens=True))

# Load the pre-trained GPT-2 model
gpt2_model = AutoModelForCausalLM.from_pretrained('gpt2')
# Generate text using the GPT-2 model
outputs = gpt2_model.generate(**inputs, generation_config=generation_config)
# Decode the generated text and print it
print("\nGPT2 response:\t",tokenizer.decode(outputs[0], skip_special_tokens=True))


# Althought the model is trained on a small data for 5 epochs only, it can be seen that the response generated by the DPO-tuned model is more concise and straightforward.
# 

# # Exercise
# 
# 

# ### Exercise 1: Preprocess the `argilla/ultrafeedback-binarized-preferences-cleaned` Dataset
# 

# This data set comprises user-generated prompts along with corresponding responses categorized as either "chosen" or "rejected." It provides a rich source of binary feedback, making it ideal for training models to align with user preferences.
# 

# ##### Load the data set from the `argilla/ultrafeedback-binarized-preferences-cleaned`
# 

# In[39]:


#TODO
dataset = load_dataset("argilla/ultrafeedback-binarized-preferences-cleaned")


# <details>
#     <summary>Click here for hint</summary>
# 
# ```python
# dataset = load_dataset("argilla/ultrafeedback-binarized-preferences-cleaned")
# ```
# 
# </details>
# 

# In[40]:


print(dataset['train'])


# ##### Set the variable cnt to 50 and then select the first 50 (cnt) examples to reduce the volume of data for resource limitations.
# 

# In[41]:


#TODO
cnt = 50  # You can adjust this count based on your requirements

# Select the first 5% of examples
dataset['train'] = dataset['train'].select(range(cnt))


# <details>
#     <summary>Click here for hint</summary>
# 
# ```python
# cnt = 50  # You can adjust this count based on your requirements
# 
# # Select the first 5% of examples
# dataset['train'] = dataset['train'].select(range(cnt))
# ```
# 
# </details>
# 

# ##### Create a function named `process` that takes a row of data as input. Within this function, remove unwanted columns such as `source, chosen-rating, chosen-model, rejected-rating, and rejected-model`. Then, use the map function to apply the process function to each row in the training data set.
# 

# In[42]:


#TODO
'''def process(row):
    # Delete unwanted columns
    del row["source"]
    del row["chosen-rating"]
    del row["chosen-model"]
    del row["rejected-rating"]
    del row["rejected-model"]
    
    # Retrieve the actual response text
    row["chosen"] = row["chosen"][-1]["content"]
    row["rejected"] = row["rejected"][-1]["content"]
    
    return row

# Apply the data processing function to the dataset
dataset['train'] = dataset['train'].map(
    process,
    num_proc=multiprocessing.cpu_count(),
    load_from_cache_file=False,
)
'''
# Convertir en liste de dictionnaires
data_list = dataset["train"].to_list()

# Appliquer la transformation
processed_data = [process(row) for row in data_list]

# Reconvertir en Dataset
dataset['train'] = Dataset.from_dict({key: [row[key] for row in processed_data] for key in processed_data[0]})


# <details>
#     <summary>Click here for hint</summary>
# 
# ```python
# def process(row):
#     # Delete unwanted columns
#     del row["source"]
#     del row["chosen-rating"]
#     del row["chosen-model"]
#     del row["rejected-rating"]
#     del row["rejected-model"]
#     
#     # Retrieve the actual response text
#     row["chosen"] = row["chosen"][-1]["content"]
#     row["rejected"] = row["rejected"][-1]["content"]
#     
#     return row
# 
# # Apply the data processing function to the dataset
# dataset['train'] = dataset['train'].map(
#     process,
#     num_proc=multiprocessing.cpu_count(),
#     load_from_cache_file=False,
# )
# ```
# 
# </details>
# 

# ##### Split the data set into training and evaluation sets:
# Calculate the size for the training set as 80% of the total data. The remaining 20% will be used for evaluation.
# 

# In[43]:


#TODO
train_size = int(0.8 * len(dataset['train']))  # 80% for training
eval_size = len(dataset['train']) - train_size  # Remaining 20% for evaluation

train_dataset = dataset['train'].select(range(train_size))
eval_dataset = dataset['train'].select(range(train_size, train_size + eval_size))


# <details>
#     <summary>Click here for hint</summary>
# 
# ```python
# train_size = int(0.8 * len(dataset['train']))  # 80% for training
# eval_size = len(dataset['train']) - train_size  # Remaining 20% for evaluation
# 
# train_dataset = dataset['train'].select(range(train_size))
# eval_dataset = dataset['train'].select(range(train_size, train_size + eval_size))
# ```
# 
# </details>
# 

# In[44]:


print(train_dataset)


# In[45]:


print(train_dataset[0])


# ### Exercise 2: Prompt Inferencing and Comparison with GPT-2
# 

# In[52]:


PROMPT = input()


# ##### Initialize the GPT-2 Tokenizer
# 

# In[53]:


#TODO
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')


# <details>
#     <summary>Click here for hint</summary>
# 
# ```python
# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
# ```
# 
# </details>
# 

# ##### Create a generation_config object to set the parameters for text generation.
# - do_sample=True    (It enables sampling, which allows for more diverse outputs.)
# - top_k=1 (It specifies the number of highest probability vocabulary tokens to consider during generation.)
# - temperature=0.1 (It controls the randomness of the output.)
# - max_new_tokens=25 (It sets the maximum number of new tokens to generate during inference.)
# - pad_token_id=tokenizer.eos_token_id (It specifies the token to use for padding.)
# 

# In[56]:


#TODO
generation_config = GenerationConfig(
    # Use sampling to generate diverse text
    do_sample=True,
    # Top-k sampling parameter: controls the number of highest probability tokens to consider
    top_k=1,
    # Temperature parameter: controls the randomness of the generated text
    temperature=0.1,
    # Maximum number of new tokens to generate
    max_new_tokens=25,
    # Use the end-of-sequence token as the padding token
    pad_token_id=tokenizer.eos_token_id
)


# <details>
#     <summary>Click here for hint</summary>
# 
# ```python
# generation_config = GenerationConfig(
#     # Use sampling to generate diverse text
#     do_sample=True,
#     # Top-k sampling parameter: controls the number of highest probability tokens to consider
#     top_k=1,
#     # Temperature parameter: controls the randomness of the generated text
#     temperature=0.1,
#     # Maximum number of new tokens to generate
#     max_new_tokens=25,
#     # Use the end-of-sequence token as the padding token
#     pad_token_id=tokenizer.eos_token_id
# )
# ```
# 
# </details>
# 

# ##### Create a function named `generate_dpo_response` that takes a prompt as input and generates a response using the DPO model (`dpo_model`).
# 

# In[57]:


#TODO
def generate_dpo_response(prompt):
    # Tokenize the prompt
    inputs = tokenizer(prompt, return_tensors='pt')

    # Generate text using the DPO model
    outputs = dpo_model.generate(**inputs, generation_config=generation_config)
    
    # Decode and return the response
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


# <details>
#     <summary>Click here for hint</summary>
# 
# ```python
# def generate_dpo_response(prompt):
#     # Tokenize the prompt
#     inputs = tokenizer(prompt, return_tensors='pt')
# 
#     # Generate text using the DPO model
#     outputs = dpo_model.generate(**inputs, generation_config=generation_config)
#     
#     # Decode and return the response
#     return tokenizer.decode(outputs[0], skip_special_tokens=True)
# ```
# 
# </details>
# 

# ##### Create another function named `generate_gpt2_response` that takes a prompt as input and generates a response using the GPT-2 model (`gpt2_model`).
# 

# In[58]:


#TODO
def generate_gpt2_response(prompt):
    # Tokenize the prompt
    inputs = tokenizer(prompt, return_tensors='pt')

    # Generate text using the GPT-2 model
    outputs = gpt2_model.generate(**inputs, generation_config=generation_config)
    
    # Decode and return the response
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


# <details>
#     <summary>Click here for hint</summary>
# 
# ```python
# def generate_gpt2_response(prompt):
#     # Tokenize the prompt
#     inputs = tokenizer(prompt, return_tensors='pt')
# 
#     # Generate text using the GPT-2 model
#     outputs = gpt2_model.generate(**inputs, generation_config=generation_config)
#     
#     # Decode and return the response
#     return tokenizer.decode(outputs[0], skip_special_tokens=True)
# ```
# 
# </details>
# 

# ##### Call both functions with a prompt and compare the responses.
# 

# In[60]:


#TODO
# Generate responses
dpo_response = generate_dpo_response(PROMPT)
gpt2_response = generate_gpt2_response(PROMPT)

# Print the responses
print("DPO response:\t", dpo_response)
print("\nGPT-2 response:\t", gpt2_response)


# 
# 
# <details>
#     <summary>Click here for hint</summary>
# 
# ```python
# # Generate responses
# dpo_response = generate_dpo_response(PROMPT)
# gpt2_response = generate_gpt2_response(PROMPT)
# 
# # Print the responses
# print("DPO response:\t", dpo_response)
# print("\nGPT-2 response:\t", gpt2_response)
# ```
# 
# </details>
# 

# # Congratulations! You have completed the lab!
# 

# ## Authors
# 

# [Fateme Akbari](https://www.linkedin.com/in/fatemeakbari/) is a Ph.D. candidate in Information Systems at McMaster University with demonstrated research experience in Machine Learning and NLP.
# 
# [Kunal Makwana](https://author.skills.network/instructors/kunal_makwana) is a Data Scientist at IBM and is currently pursuing his Master's in Computer Science at Dalhousie University.
# 

# ## References
# [DPO Trainer](https://huggingface.co/docs/trl/main/en/dpo_trainer)
# 

# © Copyright IBM Corporation. All rights reserved.
# 

# In[ ]:




